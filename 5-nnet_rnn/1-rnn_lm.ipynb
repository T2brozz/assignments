{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2183f50-3d39-4832-af72-42791571713d",
   "metadata": {},
   "source": [
    "# Assignment 5: Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996891f9-c12d-47bb-93f5-2f25cc60709b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Task 1) RNN as Language Model\n",
    "\n",
    "Similar to the n-gram language models in the previous tasks, imagine you have to write another thesis and just want to generate an interesting topic.\n",
    "In this assignment, you will train and use Recurrent Neural Networks as language models to generate new potential thesis topics.\n",
    "\n",
    "### Data\n",
    "\n",
    "Download the `theses.csv` data set from the `Supplemental Materials` in the `Files` section of our Microsoft Teams group.\n",
    "This dataset consists of approx. 3,000 theses topics chosen by students in the past.\n",
    "Here are some examples of the file content:\n",
    "\n",
    "```\n",
    "27.10.94;14.07.95;1995;intern;Diplom;DE;Monte Carlo-Simulation für ein gekoppeltes Round-Robin-System;\n",
    "04.11.94;14.03.95;1995;intern;Diplom;DE;Implementierung eines Testüberdeckungsgrad-Analysators für RAS;\n",
    "01.11.20;01.04.21;2021;intern;Bachelor;DE;Landessprachenerkennung mittels X-Vektoren und Meta-Klassifikation;\n",
    "```\n",
    "\n",
    "### Basic Setup\n",
    "\n",
    "For the assignment on Recurrent Neural Networks, we'll (again) heavily use [PyTorch](https://pytorch.org) as go-to Deep Learning library.\n",
    "Here, we'll rely on the RNN and Embedding modules already implemented by PyTorch.\n",
    "You can imagine the Embedding layer as a simple lookup table that stores embeddings of a fixed dictionary and size (quite similar to the Word2Vec parameters we've trained in assignment 2).\n",
    "Head over to the [RNN](https://pytorch.org/docs/stable/generated/torch.nn.RNN.html) and [Embedding](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html) modules to gain some understanding of their functionality.\n",
    "Code for processing data samples, batching, converting to tensors, etc. can get messy and hard to maintain. \n",
    "Therefore, you can use PyTorch's [Datasets & DataLoaders](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html). \n",
    "Get familiar with the basics of data handling, as it will help you for upcoming assignments.\n",
    "As always, you can use [NumPy](https://numpy.org) and [Pandas](https://pandas.pydata.org) for data handling etc.\n",
    "\n",
    "*In this Jupyter Notebook, we will provide the steps to solve this task and give hints via functions & comments. However, code modifications (e.g., function naming, arguments) and implementation of additional helper functions & classes are allowed. The code aims to help you get started.*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb47c663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy\n",
      "  Using cached numpy-2.2.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-1.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Collecting torch\n",
      "  Using cached torch-2.7.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (29 kB)\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.22.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n",
      "Collecting tqdm\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting scipy>=1.6.0 (from scikit-learn)\n",
      "  Downloading scipy-1.15.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Using cached joblib-1.5.0-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Using cached threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Using cached contourpy-1.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.5 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.58.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (104 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Using cached kiwisolver-1.4.8-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/yannes/Documents/seqlrnass/.venv/lib/python3.10/site-packages (from matplotlib) (25.0)\n",
      "Collecting pillow>=8 (from matplotlib)\n",
      "  Using cached pillow-11.2.1-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (8.9 kB)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib)\n",
      "  Using cached pyparsing-3.2.3-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/yannes/Documents/seqlrnass/.venv/lib/python3.10/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Collecting filelock (from torch)\n",
      "  Using cached filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/yannes/Documents/seqlrnass/.venv/lib/python3.10/site-packages (from torch) (4.13.2)\n",
      "Collecting sympy>=1.13.3 (from torch)\n",
      "  Using cached sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch)\n",
      "  Using cached networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting jinja2 (from torch)\n",
      "  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting fsspec (from torch)\n",
      "  Using cached fsspec-2025.3.2-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.6.77 (from torch)\n",
      "  Using cached nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.6.77 (from torch)\n",
      "  Using cached nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.6.80 (from torch)\n",
      "  Using cached nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.5.1.17 (from torch)\n",
      "  Using cached nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.6.4.1 (from torch)\n",
      "  Using cached nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.3.0.4 (from torch)\n",
      "  Using cached nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.7.77 (from torch)\n",
      "  Using cached nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.7.1.2 (from torch)\n",
      "  Using cached nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.5.4.2 (from torch)\n",
      "  Using cached nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparselt-cu12==0.6.3 (from torch)\n",
      "  Using cached nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting nvidia-nccl-cu12==2.26.2 (from torch)\n",
      "  Using cached nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.6.77 (from torch)\n",
      "  Using cached nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.6.85 (from torch)\n",
      "  Using cached nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufile-cu12==1.11.1.6 (from torch)\n",
      "  Using cached nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting triton==3.3.0 (from torch)\n",
      "  Using cached triton-3.3.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting setuptools>=40.8.0 (from triton==3.3.0->torch)\n",
      "  Using cached setuptools-80.4.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: six>=1.5 in /home/yannes/Documents/seqlrnass/.venv/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch)\n",
      "  Using cached MarkupSafe-3.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
      "Using cached numpy-2.2.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.4 MB)\n",
      "Using cached scikit_learn-1.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.5 MB)\n",
      "Downloading matplotlib-3.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached torch-2.7.0-cp310-cp310-manylinux_2_28_x86_64.whl (865.2 MB)\n",
      "Using cached nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (393.1 MB)\n",
      "Using cached nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.9 MB)\n",
      "Using cached nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl (23.7 MB)\n",
      "Using cached nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (897 kB)\n",
      "Using cached nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl (571.0 MB)\n",
      "Using cached nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (200.2 MB)\n",
      "Using cached nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.1 MB)\n",
      "Using cached nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (56.3 MB)\n",
      "Using cached nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (158.2 MB)\n",
      "Using cached nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (216.6 MB)\n",
      "Using cached nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl (156.8 MB)\n",
      "Using cached nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (201.3 MB)\n",
      "Using cached nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (19.7 MB)\n",
      "Using cached nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
      "Using cached triton-3.3.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (156.4 MB)\n",
      "Downloading torchvision-0.22.0-cp310-cp310-manylinux_2_28_x86_64.whl (7.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Using cached contourpy-1.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (325 kB)\n",
      "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.58.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached joblib-1.5.0-py3-none-any.whl (307 kB)\n",
      "Using cached kiwisolver-1.4.8-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n",
      "Using cached pillow-11.2.1-cp310-cp310-manylinux_2_28_x86_64.whl (4.6 MB)\n",
      "Using cached pyparsing-3.2.3-py3-none-any.whl (111 kB)\n",
      "Downloading scipy-1.15.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.7/37.7 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached setuptools-80.4.0-py3-none-any.whl (1.2 MB)\n",
      "Using cached sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Using cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Using cached filelock-3.18.0-py3-none-any.whl (16 kB)\n",
      "Using cached fsspec-2025.3.2-py3-none-any.whl (194 kB)\n",
      "Using cached jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Using cached MarkupSafe-3.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (20 kB)\n",
      "Using cached networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "Installing collected packages: nvidia-cusparselt-cu12, mpmath, tqdm, threadpoolctl, sympy, setuptools, pyparsing, pillow, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, MarkupSafe, kiwisolver, joblib, fsspec, fonttools, filelock, cycler, triton, scipy, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, jinja2, contourpy, scikit-learn, nvidia-cusolver-cu12, matplotlib, torch, torchvision\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38/38\u001b[0m [torchvision]\u001b[0m [torchvision]ver-cu12]2]2]\n",
      "\u001b[1A\u001b[2KSuccessfully installed MarkupSafe-3.0.2 contourpy-1.3.2 cycler-0.12.1 filelock-3.18.0 fonttools-4.58.0 fsspec-2025.3.2 jinja2-3.1.6 joblib-1.5.0 kiwisolver-1.4.8 matplotlib-3.10.3 mpmath-1.3.0 networkx-3.4.2 numpy-2.2.5 nvidia-cublas-cu12-12.6.4.1 nvidia-cuda-cupti-cu12-12.6.80 nvidia-cuda-nvrtc-cu12-12.6.77 nvidia-cuda-runtime-cu12-12.6.77 nvidia-cudnn-cu12-9.5.1.17 nvidia-cufft-cu12-11.3.0.4 nvidia-cufile-cu12-1.11.1.6 nvidia-curand-cu12-10.3.7.77 nvidia-cusolver-cu12-11.7.1.2 nvidia-cusparse-cu12-12.5.4.2 nvidia-cusparselt-cu12-0.6.3 nvidia-nccl-cu12-2.26.2 nvidia-nvjitlink-cu12-12.6.85 nvidia-nvtx-cu12-12.6.77 pillow-11.2.1 pyparsing-3.2.3 scikit-learn-1.6.1 scipy-1.15.3 setuptools-80.4.0 sympy-1.14.0 threadpoolctl-3.6.0 torch-2.7.0 torchvision-0.22.0 tqdm-4.67.1 triton-3.3.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install numpy scikit-learn matplotlib torch torchvision tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d1dab80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Using cached pandas-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /home/yannes/Documents/seqlrnass/.venv/lib/python3.10/site-packages (from pandas) (2.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/yannes/Documents/seqlrnass/.venv/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Using cached tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in /home/yannes/Documents/seqlrnass/.venv/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Using cached pandas-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
      "Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Installing collected packages: pytz, tzdata, pandas\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/3\u001b[0m [pandas]2m2/3\u001b[0m [pandas]\n",
      "\u001b[1A\u001b[2KSuccessfully installed pandas-2.2.3 pytz-2025.2 tzdata-2025.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ffb0527-90ea-4f0a-ab0c-40817df51dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import os\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf666267-390d-402a-aae9-e3588b51c262",
   "metadata": {},
   "source": [
    "### Prepare the Data\n",
    "\n",
    "1.1 Spend some time on preparing the dataset. It may be helpful to lower-case the data and to filter for German titles. The format of the CSV-file should be:\n",
    "\n",
    "```\n",
    "Anmeldedatum;Abgabedatum;JahrAkademisch;Art;Grad;Sprache;Titel;Abstract\n",
    "```\n",
    "\n",
    "1.2 Create the vocabulary from the prepared dataset. You'll need it for the modeling part such as nn.Embedding.\n",
    "\n",
    "1.3 Create a PyTorch Dataset class which handles your tokenized data with respect to model inputs and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8c9e357",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_theses_dataset(filepath):\n",
    "    \"\"\"Loads all theses instances and returns them as a dataframe.\"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "   \n",
    "    \n",
    "    \n",
    "    data= pd.read_csv(filepath, sep='\\t', encoding='utf-8', header=None)\n",
    "    data = data[3].to_list()\n",
    "    return data\n",
    "    \n",
    "    ### END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fa699c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(dataframe):\n",
    "    \"\"\"Preprocesses and tokenizes the given theses titles for further use.\"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    # Tokenize the titles\n",
    "    # Remove special characters\n",
    "    # Convert to lowercase\n",
    "\n",
    "    for i in range(len(dataframe)):\n",
    "        dataframe[i] = dataframe[i].lower()\n",
    "        dataframe[i] = dataframe[i].replace(\"'\", \"\")\n",
    "        dataframe[i] = dataframe[i].replace('\"', \"\")\n",
    "        dataframe[i] = dataframe[i].replace(\"(\", \"\")\n",
    "        dataframe[i] = dataframe[i].replace(\")\", \"\")\n",
    "        dataframe[i] = dataframe[i].replace(\",\", \"\")\n",
    "        dataframe[i] = dataframe[i].replace(\".\", \"\")\n",
    "        dataframe[i] = dataframe[i].replace(\"!\", \"\")\n",
    "        dataframe[i] = dataframe[i].replace(\"?\", \"\")\n",
    "        dataframe[i] = dataframe[i].replace(\":\", \"\")\n",
    "        dataframe[i] = dataframe[i].replace(\";\", \"\")\n",
    "        dataframe[i] = dataframe[i].replace(\"-\", \" \")\n",
    "        dataframe[i] = dataframe[i].replace(\"_\", \" \")   \n",
    "        dataframe[i] = dataframe[i].replace(\"  \", \" \")\n",
    "\n",
    "    # Tokenize the titles\n",
    "    for i in range(len(dataframe)):\n",
    "        dataframe[i] = dataframe[i].split(\" \")\n",
    "        # add start and end tokens\n",
    "        dataframe[i].insert(0, \"<s>\")\n",
    "        dataframe[i].append(\"</s>\")\n",
    "    # Remove empty strings\n",
    "    for i in range(len(dataframe)):\n",
    "        dataframe[i] = list(filter(None, dataframe[i]))\n",
    "    \n",
    "\n",
    "\n",
    "    ### END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "299d6af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cerate_vocab(dataframe):\n",
    "    \"\"\"Creates a vocabulary from the given dataframe.\"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    # Create a vocabulary from the tokenized titles\n",
    "    vocab = set()\n",
    "    for title in dataframe:\n",
    "        for word in title:\n",
    "            vocab.add(word)\n",
    "    return vocab\n",
    "    ### END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59987798",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5d225b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = load_theses_dataset(\"data/theses.tsv\")\n",
    "preprocess(dataframe)\n",
    "\n",
    "\n",
    "vocabulary = cerate_vocab(dataframe)\n",
    "word2idx = {word: idx for idx, word in enumerate(vocabulary)}\n",
    "idx2word = {idx: word for idx, word in enumerate(vocabulary)}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dbc975e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThesesDataset(Dataset):\n",
    "    def __init__(self, dataset, word2idx):\n",
    "        \"\"\"\n",
    "        Initializes the dataset.\n",
    "        Args:\n",
    "            dataset (list of list of str): Tokenized theses titles.\n",
    "            word2idx (dict): Mapping of words to their indices.\n",
    "        \"\"\"\n",
    "        self.data = []\n",
    "        self.labels = []\n",
    "        \n",
    "        for title in dataset:\n",
    "            # Convert words to indices\n",
    "            indices = [word2idx[word] for word in title[:-1]]  # All words except the last one\n",
    "            label_indices = [word2idx[word] for word in title[1:]]  # All words except the first one\n",
    "            \n",
    "            self.data.append(indices)\n",
    "            self.labels.append(label_indices)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the number of samples in the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Returns the sample and label at the given index.\n",
    "        Args:\n",
    "            idx (int): Index of the sample.\n",
    "        Returns:\n",
    "            tuple: (sample, label) where both are lists of word indices.\n",
    "        \"\"\"\n",
    "        sample = torch.tensor(self.data[idx], dtype=torch.long)\n",
    "        labels = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return sample, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3dfd9324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 2979\n",
      "(tensor([4366, 6021, 2135, 7205, 5187, 5226, 4112]), tensor([6021, 2135, 7205, 5187, 5226, 4112, 5987]))\n"
     ]
    }
   ],
   "source": [
    "dataset = ThesesDataset(dataframe, word2idx)\n",
    "print(f\"Dataset size: {len(dataset)}\")\n",
    "print(dataset.__getitem__(0))  # Print the first sample and its label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ffb8bb6",
   "metadata": {},
   "source": [
    "### Train and Evaluate\n",
    "\n",
    "2.1 Implement the RNN Language Model. Therefore, you can use the nn.Module and overwrite the forward function. For the embedding layer you can either use the embeddings learned from the previous word2vec assignment or train the `nn.Embedding` module and corresponding parameters from scratch.\n",
    "\n",
    "2.2 Implement the functionality to train your model with the train dataset.\n",
    "\n",
    "2.3 Implement the functionality to evaluate your model with the test dataset.\n",
    "\n",
    "2.4 Perform a train-test-split for your theses data, train the RNN Language Model and evaluate the loss & perplexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d309048",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_LM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers=1):\n",
    "        \"\"\"\n",
    "        Initializes the RNN Language Model.\n",
    "        Args:\n",
    "            vocab_size (int): Size of the vocabulary.\n",
    "            embedding_dim (int): Dimension of the word embeddings.\n",
    "            hidden_dim (int): Number of hidden units in the RNN.\n",
    "            num_layers (int): Number of RNN layers.\n",
    "        \"\"\"\n",
    "        super(RNN_LM, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, X, hidden=None):\n",
    "        \"\"\"\n",
    "        Forward pass of the RNN Language Model.\n",
    "        Args:\n",
    "            X (Tensor): Input tensor of shape (batch_size, sequence_length).\n",
    "            hidden (Tensor, optional): Hidden state tensor of shape (num_layers, batch_size, hidden_dim).\n",
    "        Returns:\n",
    "            output (Tensor): Output tensor of shape (batch_size, sequence_length, vocab_size).\n",
    "            hidden (Tensor): Hidden state tensor of shape (num_layers, batch_size, hidden_dim).\n",
    "        \"\"\"\n",
    "        embedded = self.embedding(X)  # Shape: (batch_size, sequence_length, embedding_dim)\n",
    "        output, hidden = self.rnn(embedded, hidden)  # RNN output and hidden state\n",
    "        output = self.fc(output)  # Shape: (batch_size, sequence_length, vocab_size)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a65b62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: 2.2 Implement the train functionality\n",
    "### Notice: If you want, you can also combine train and eval functionality\n",
    "\n",
    "def train(arguments):\n",
    "    \"\"\"Trains the RNN-LM for one epoch.\"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    ### END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02549c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: 2.3 Implement the evaluation functionality\n",
    "### Notice: If you want, you can also combine train and eval\n",
    "\n",
    "def eval(arguments):\n",
    "    \"\"\"Evaluates the optimized RNN-LM.\"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    ### END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5fa1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: 2.4 Initialize and train the RNN Language Model for X epochs\n",
    "\n",
    "# For split reproducibility\n",
    "# Optional: Use 5-fold cross validation\n",
    "SEED = 42\n",
    "\n",
    "EPOCHS = 100\n",
    "\n",
    "DEVICE = \"cpu\" # 'cpu', 'mps' or 'cuda'\n",
    "\n",
    "### YOUR CODE HERE\n",
    "\n",
    "# Use batch_size=1 if you want to avoid padding handling\n",
    "train_dataset = None\n",
    "train_dataloader = None\n",
    "\n",
    "# Use batch_size=1 if you want to avoid padding handling\n",
    "test_dataset = None\n",
    "test_dataloader = None\n",
    "\n",
    "# Your language model\n",
    "model = None\n",
    "\n",
    "# Your loss function\n",
    "criterion = None\n",
    "\n",
    "# Your optimizer (optim.SGD should be okay)\n",
    "optimizer = None\n",
    "\n",
    "\n",
    "# TODO: Training for epoch i\n",
    "\n",
    "# TODO: Evaluation for epoch i\n",
    "\n",
    "\n",
    "### END YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe48a9b",
   "metadata": {},
   "source": [
    "### Generate Titles\n",
    "\n",
    "3.1 Use the trained RNN Language Model to generate theses titles. How can you sample the next tokens?\n",
    "\n",
    "3.2 Compare your results with n-gram language models (e.g., n=4). Of course, you can use a library such as NLTK toolkit\n",
    "- What perplexity does a regular 4-gram have on the same split? \n",
    "- Compare the generated titles from the 4-gram and RNN-LM. Do you think the n-gram titles are better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3eb891f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: 3.1 Generate titles with the trained RNN Language Model\n",
    "\n",
    "def generate(arguments):\n",
    "    ### YOUR CODE HERE\n",
    "\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    ### END YOUR CODE\n",
    "\n",
    "for i in range(10):\n",
    "    generated_title = generate(None)\n",
    "    print(\" \".join(generated_title))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749babb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: 3.2 Generate titles with the trained n-gram language model\n",
    "\n",
    "### YOUR CODE HERE\n",
    "\n",
    "\n",
    "\n",
    "### END YOUR CODE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
