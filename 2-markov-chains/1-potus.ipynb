{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2183f50-3d39-4832-af72-42791571713d",
   "metadata": {},
   "source": [
    "# Assignment 2: POTUS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996891f9-c12d-47bb-93f5-2f25cc60709b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Task 1) President of the United States (Trump vs. Obama)\n",
    "\n",
    "Surely, you're aware that the 45th President of the United States (@POTUS45) was an active user of Twitter, until (permanently) banned on Jan 8, 2021.\n",
    "You can still enjoy his greatness at the [Trump Twitter Archive](https://www.thetrumparchive.com/). We will be using original tweets only, so make sure to remove all retweets.\n",
    "Another fan of Twitter was Barack Obama (@POTUS43 and @POTUS44), who used the platform in a rather professional way.\n",
    "Please also consider the POTUS Tweets of Joe Biden; we will be using those for testing.\n",
    "\n",
    "### Data\n",
    "\n",
    "There are multiple ways to get the data, but the easiest way is to download the files from the `Supplemental Materials` in the `Files` section of our Microsoft Teams group. \n",
    "Another way is to directly use the data from [Trump Twitter Archive](https://www.thetrumparchive.com/), [Obama Kaggle](https://www.kaggle.com/jayrav13/obama-white-house), and [Biden Kaggle](https://www.kaggle.com/rohanrao/joe-biden-tweets).\n",
    "Before you get started, please download the files; you can put them into the data folder.\n",
    "\n",
    "### N-gram Models\n",
    "\n",
    "In this assignment, you will be doing some Twitter-related preprocessing and training n-gram models to be able to distinguish between Tweets of Trump, Obama, and Biden.\n",
    "We will be using [NLTK](https://www.nltk.org), more specifically it's [`lm`](https://www.nltk.org/api/nltk.lm.html) module. \n",
    "Install the NLTK package within your working environment.\n",
    "You can use some of the NLTK functions, but you have to implement the functions for likelihoods and perplexity from scratch.\n",
    "\n",
    "*In this Jupyter Notebook, we will provide the steps to solve this task and give hints via functions & comments. However, code modifications (e.g., function naming, arguments) and implementation of additional helper functions & classes are allowed. The code aims to help you get started.*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a4a980f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting click (from nltk)\n",
      "  Using cached click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: joblib in /home/yannes/Documents/seqlrn/.venv/lib64/python3.13/site-packages (from nltk) (1.4.2)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Downloading regex-2024.11.6-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Collecting tqdm (from nltk)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading regex-2024.11.6-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (796 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m796.9/796.9 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached click-8.1.8-py3-none-any.whl (98 kB)\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm, regex, click, nltk\n",
      "Successfully installed click-8.1.8 nltk-3.9.1 regex-2024.11.6 tqdm-4.67.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ffb0527-90ea-4f0a-ab0c-40817df51dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import regex as re\n",
    "import sklearn as sk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf666267-390d-402a-aae9-e3588b51c262",
   "metadata": {},
   "source": [
    "### Prepare the Data\n",
    "\n",
    "1.1 Prepare all the Tweets. Since the `lm` modules will work on tokenized data, implement a tokenization method that strips unnecessary tokens but retains special words such as mentions (@...) and hashtags (#...).\n",
    "\n",
    "1.2 Partition into training and test sets; select about 100 tweets each, which we will be testing on later. As with any Machine Learning task, training and test must not overlap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01c50114",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /home/yannes/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2ae5b5d-fccd-4092-af20-d8e8b4a65ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice: ignore retweets \n",
    "def beautify_tweet(tweet):\n",
    "    \"\"\"Returns a cleaned version of the tweet.\"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    # remove special charactoars but keep @ and # \n",
    "    cleaned_tweet = re.sub(r\"[^a-zA-Z\\s@#]\", \"\", tweet)\n",
    "    \n",
    "    # remove extra spaces\n",
    "    cleaned_tweet = re.sub(r\"\\s+\", \" \", cleaned_tweet)\n",
    "    \n",
    "    # remove leading and trailing spaces\n",
    "    cleaned_tweet = cleaned_tweet.strip()\n",
    "\n",
    "    # remove https links\n",
    "    cleaned_tweet = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", cleaned_tweet, flags=re.MULTILINE)\n",
    "    \n",
    "    return cleaned_tweet\n",
    "    ### END YOUR CODE\n",
    "\n",
    "def load_trump_tweets(filepath):\n",
    "    \"\"\"Loads all Trump tweets and returns them as a list.\"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    \n",
    "    data = pd.read_json(filepath)\n",
    "    data= data['text'].tolist()\n",
    "    # remove special charactoars but keep @ and # \n",
    "    cleaned_tweets = [       beautify_tweet(tweet) for tweet in data]\n",
    "    \n",
    "    return cleaned_tweets\n",
    "\n",
    "    ### END YOUR CODE\n",
    "\n",
    "\n",
    "def load_obama_tweets(filepath):\n",
    "    \"\"\"Loads all Obama tweets and returns them as a list.\"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    \n",
    "    data= pd.read_csv(filepath)\n",
    "    data = data[\"Tweet-text\"].to_list()\n",
    "    cleaned_tweets = [       beautify_tweet(tweet) for tweet in data]\n",
    "    \n",
    "    return cleaned_tweets\n",
    "\n",
    "\n",
    "    ### END YOUR CODE\n",
    "    \n",
    "\n",
    "def load_biden_tweets(filepath):\n",
    "    \"\"\"Loads all Biden tweets and returns them as a list.\"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    \n",
    "    data= pd.read_csv(filepath)\n",
    "    data = data[\"tweet\"].to_list()\n",
    "    cleaned_tweets = [       beautify_tweet(tweet) for tweet in data]\n",
    "    \n",
    "    return cleaned_tweets\n",
    "\n",
    "\n",
    "\n",
    "    ### END YOUR CODE\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f25e8c56-3837-440b-bebe-1916ebede6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Notice: think about start and end tokens\n",
    "\n",
    "NUM_TEST = 100\n",
    "\n",
    "def tokenize(text):\n",
    "    \"\"\"Tokenizes a single Tweet.\"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    \n",
    "    # Tokenize the text\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    # Add start and end tokens\n",
    "    tokens = [\"<s>\"] + tokens + [\"</s>\"]\n",
    "    \n",
    "    return tokens\n",
    "    ### END YOUR CODE\n",
    "    \n",
    "\n",
    "def split_and_tokenize(data, num_test=NUM_TEST):\n",
    "    \"\"\"Splits and tokenizes the given list of Twitter tweets.\"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    \n",
    "    #train , test= sk.model_selection.train_test_split(data, test_size=num_test)\n",
    "    train = [tokenize(tweet) for tweet in data]\n",
    "    #test = [tokenize(tweet) for tweet in test]\n",
    "    \n",
    "    return train\n",
    "    ### END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2598cfcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "trump_tweets_train  = split_and_tokenize(load_trump_tweets(\"data/trump.json\"))\n",
    "obama_tweets_train  = split_and_tokenize(load_obama_tweets(\"data/obama.csv\"))\n",
    "biden_tweets_train  = split_and_tokenize(load_biden_tweets(\"data/biden.csv\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c32c33-1a19-42ac-93d0-fcf66fbeaf5f",
   "metadata": {},
   "source": [
    "### Train N-gram Models\n",
    "\n",
    "2.1 Train n-gram models with n = [1, ..., 5] for Obama, Trump, and Biden.\n",
    "\n",
    "2.2 Also train a joint model, that will serve as background model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8866e738",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['<s>',\n",
       "  'Republicans',\n",
       "  'and',\n",
       "  'Democrats',\n",
       "  'have',\n",
       "  'both',\n",
       "  'created',\n",
       "  'our',\n",
       "  'economic',\n",
       "  'problems',\n",
       "  '</s>'],\n",
       " ['<s>',\n",
       "  'I',\n",
       "  'was',\n",
       "  'thrilled',\n",
       "  'to',\n",
       "  'be',\n",
       "  'back',\n",
       "  'in',\n",
       "  'the',\n",
       "  'Great',\n",
       "  'city',\n",
       "  'of',\n",
       "  'Charlotte',\n",
       "  'North',\n",
       "  'Carolina',\n",
       "  'with',\n",
       "  'thousands',\n",
       "  'of',\n",
       "  'hardworking',\n",
       "  'American',\n",
       "  'Patriots',\n",
       "  'who',\n",
       "  'love',\n",
       "  'our',\n",
       "  'Country',\n",
       "  'cherish',\n",
       "  'our',\n",
       "  'values',\n",
       "  'respect',\n",
       "  'our',\n",
       "  'laws',\n",
       "  'and',\n",
       "  'always',\n",
       "  'put',\n",
       "  'AMERICA',\n",
       "  'FIRST',\n",
       "  'Thank',\n",
       "  'you',\n",
       "  'for',\n",
       "  'a',\n",
       "  'wonderful',\n",
       "  'evening',\n",
       "  '#',\n",
       "  'KAG',\n",
       "  '</s>'],\n",
       " ['<s>',\n",
       "  'RT',\n",
       "  '@',\n",
       "  'CBSHerridge',\n",
       "  'READ',\n",
       "  'Letter',\n",
       "  'to',\n",
       "  'surveillance',\n",
       "  'court',\n",
       "  'obtained',\n",
       "  'by',\n",
       "  'CBS',\n",
       "  'News',\n",
       "  'questions',\n",
       "  'where',\n",
       "  'there',\n",
       "  'will',\n",
       "  'be',\n",
       "  'further',\n",
       "  'disciplinary',\n",
       "  'action',\n",
       "  'and',\n",
       "  'cho',\n",
       "  '</s>'],\n",
       " ['<s>',\n",
       "  'The',\n",
       "  'Unsolicited',\n",
       "  'Mail',\n",
       "  'In',\n",
       "  'Ballot',\n",
       "  'Scam',\n",
       "  'is',\n",
       "  'a',\n",
       "  'major',\n",
       "  'threat',\n",
       "  'to',\n",
       "  'our',\n",
       "  'Democracy',\n",
       "  'amp',\n",
       "  'the',\n",
       "  'Democrats',\n",
       "  'know',\n",
       "  'it',\n",
       "  'Almost',\n",
       "  'all',\n",
       "  'recent',\n",
       "  'elections',\n",
       "  'using',\n",
       "  'this',\n",
       "  'system',\n",
       "  'even',\n",
       "  'though',\n",
       "  'much',\n",
       "  'smaller',\n",
       "  'amp',\n",
       "  'with',\n",
       "  'far',\n",
       "  'fewer',\n",
       "  'Ballots',\n",
       "  'to',\n",
       "  'count',\n",
       "  'have',\n",
       "  'ended',\n",
       "  'up',\n",
       "  'being',\n",
       "  'a',\n",
       "  'disaster',\n",
       "  'Large',\n",
       "  'numbers',\n",
       "  'of',\n",
       "  'missing',\n",
       "  'Ballots',\n",
       "  'amp',\n",
       "  'Fraud',\n",
       "  '</s>'],\n",
       " ['<s>',\n",
       "  'RT',\n",
       "  '@',\n",
       "  'MZHemingway',\n",
       "  'Very',\n",
       "  'friendly',\n",
       "  'telling',\n",
       "  'of',\n",
       "  'events',\n",
       "  'here',\n",
       "  'about',\n",
       "  'Comeys',\n",
       "  'apparent',\n",
       "  'leaking',\n",
       "  'to',\n",
       "  'compliant',\n",
       "  'media',\n",
       "  'If',\n",
       "  'you',\n",
       "  'read',\n",
       "  'those',\n",
       "  'articles',\n",
       "  'and',\n",
       "  'tho',\n",
       "  '</s>']]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trump_tweets_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0828bda0-cef2-428b-a238-7f07f2c25425",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def build_n_gram_models(n, data):\n",
    "    \"\"\"\n",
    "    To predict the first few words of the Tweet, we need the smaller n-grams as\n",
    "    well. This method does calculate all n-grams up to the given n.\n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    \n",
    "    n_gram_models = {}\n",
    "\n",
    "    for i in range(1, n+1):\n",
    "        n_gram_model = {}\n",
    "        for tweet in data:\n",
    "            for j in range(len(tweet)-i):\n",
    "                n_gram = tuple(tweet[j:j+i])\n",
    "                if n_gram not in n_gram_model:\n",
    "                    n_gram_model[n_gram] = []\n",
    "                n_gram_model[n_gram].append(tweet[j+i])\n",
    "\n",
    "        # Store the model\n",
    "        n_gram_models[i] = n_gram_model\n",
    "    return n_gram_models\n",
    "\n",
    "    ### END YOUR CODE\n",
    "\n",
    "\n",
    "def get_suggestion(prev, n_gram_model):\n",
    "    \"\"\"\n",
    "    Gets the next random word for the given n_grams.\n",
    "    The size of the previous tokens must be exactly one less than the n-value\n",
    "    of the n-gram, or it will not be able to make a prediction.\n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    \n",
    "    # Check if the previous tokens are in the n-gram model\n",
    "    if tuple(prev) in n_gram_model:\n",
    "        # Get the next word\n",
    "        next_words = n_gram_model[tuple(prev)]\n",
    "        # Choose a random word from the list\n",
    "        return next_words[0]  # Replace with random choice if needed\n",
    "    else:\n",
    "        return None  # No suggestion available for the given previous tokens\n",
    "\n",
    "    ### END YOUR CODE\n",
    "\n",
    "\n",
    "def get_random_tweet(n, n_gram_models):\n",
    "    \"\"\"Generates a random tweet using the given data set.\"\"\"\n",
    "    model = n_gram_models[n]\n",
    "    start_ngram = random.choice(list(model.keys()))\n",
    "    tweet = list(start_ngram)\n",
    "    tweet =tweet[0:1]\n",
    "    print(\"start \", tweet)\n",
    "    while True:\n",
    "        prev = tweet[-(n-1):]\n",
    "        if len(prev) < n:\n",
    "            next_word = get_suggestion(prev, n_gram_models[len(prev)])\n",
    "        else:\n",
    "            next_word = get_suggestion(prev, model)\n",
    "        if next_word is None:\n",
    "            break\n",
    "        tweet.append(next_word)\n",
    "    return \" \".join(tweet)\n",
    "    ### END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4a034f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "37e5cc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_gram_models_trump = build_n_gram_models(5, trump_tweets_train)\n",
    "n_gram_models_biden = build_n_gram_models(5, biden_tweets_train)\n",
    "n_gram_models_obama = build_n_gram_models(5, obama_tweets_train)\n",
    "\n",
    "##random_tweet_trump = get_random_tweet(4, n_gram_models)\n",
    "#print(random_tweet_trump)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c11c77d9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'n_gram_models' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m get_suggestion([\u001b[33m\"\u001b[39m\u001b[33m<s>\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mIn\u001b[39m\u001b[33m\"\u001b[39m], \u001b[43mn_gram_models\u001b[49m[\u001b[32m2\u001b[39m])\n",
      "\u001b[31mNameError\u001b[39m: name 'n_gram_models' is not defined"
     ]
    }
   ],
   "source": [
    "get_suggestion([\"<s>\",\"In\"], n_gram_models[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "52dada96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start  ['sure']\n",
      "sure that the rules of democracy are faireverywherebecause the next generationand doing the hard work to get us there </s>\n"
     ]
    }
   ],
   "source": [
    "random_tweet_trump = get_random_tweet(3, n_gram_models_obama)\n",
    "print(random_tweet_trump)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648216bb-a49e-45ff-bb8c-9094c33acc07",
   "metadata": {},
   "source": [
    "### Classify the Tweets\n",
    "\n",
    "3.1 Use the log-ratio method to classify the Tweets for Trump vs. Biden. Trump should be easy to spot; but what about Obama vs. Biden?\n",
    "\n",
    "3.2 Analyze: At what context length (n) does the system perform best?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "99dd4ca5-8094-40b0-aa1b-a51268659397",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def calculate_single_token_log_ratio(prev, token, n_gram_model1, n_gram_model2):\n",
    "    \"\"\"\n",
    "    Calculates the log ratio:\n",
    "      log( P(token|prev, model1) / P(token|prev, model2) )\n",
    "    using frequency counts from the two provided n-gram models.\n",
    "    Smoothing is applied if the context is unseen.\n",
    "    \"\"\"\n",
    "    context = tuple(prev)\n",
    "    \n",
    "    # Model 1 counts\n",
    "    if context in n_gram_model1:\n",
    "        occurrences1 = n_gram_model1[context]\n",
    "        count1 = occurrences1.count(token)\n",
    "        total1 = len(occurrences1)\n",
    "    else:\n",
    "        count1, total1 = 0, 0\n",
    "        \n",
    "    # Model 2 counts\n",
    "    if context in n_gram_model2:\n",
    "        occurrences2 = n_gram_model2[context]\n",
    "        count2 = occurrences2.count(token)\n",
    "        total2 = len(occurrences2)\n",
    "    else:\n",
    "        count2, total2 = 0, 0\n",
    "    \n",
    "    smoothing = 1e-8  # Small constant to avoid division by zero\n",
    "    prob1 = (count1 + smoothing) / (total1 + smoothing) if total1 > 0 else smoothing\n",
    "    prob2 = (count2 + smoothing) / (total2 + smoothing) if total2 > 0 else smoothing\n",
    "    \n",
    "    return math.log(prob1 / prob2)\n",
    "\n",
    "\n",
    "\n",
    "def classify(n, tokens, n_gram_models1, n_gram_models2):\n",
    "    \"\"\"\n",
    "    Checks which of the two given datasets is more likely for the given Tweet.\n",
    "    If true is returned, the first one is more likely, otherwise the second.\n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    rations=[]\n",
    "    for i, token in enumerate(tokens.split()):\n",
    "        context = tokens.split()[max(0, i - n ):i]\n",
    "        n_gram_models_size=n\n",
    "        if len(context) < n:\n",
    "            n_gram_models_size=len(context) # n-gram Kontext\n",
    "        ratio = calculate_single_token_log_ratio(context, token, n_gram_models1[n], n_gram_models2[n])\n",
    "        print(f\"Token: {token}, Context: {context}, Ratio: {ratio}\")\n",
    "        rations.append(ratio)\n",
    "    if sum(rations) > 1:\n",
    "        return f\"First with {sum(rations)}\"\n",
    "    else:\n",
    "        return f\"Second with {sum(rations)}\"\n",
    "    \n",
    "    ### END YOUR CODE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861dace7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_single_token_log_ratio(['strategy', 'to', 'secure'], \"GOP\", n_gram_models_trump[3], n_gram_models_biden[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "97d1e9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(n, data1, data2, classify_fn):\n",
    "    \"\"\"\n",
    "    Trains the n-gram models on the train data and validates on the test data.\n",
    "    Uses the implemented classification function to predict the Tweeter.\n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    train , test= sk.model_selection.train_test_split(data1, test_size=NUM_TEST)\n",
    "\n",
    "    # Build n-gram models for both datasets\n",
    "    n_gram_models1 = build_n_gram_models(n, train)\n",
    "    n_gram_models2 = build_n_gram_models(n, data2)\n",
    "    n_gram_models_test = build_n_gram_models(n, test)\n",
    "    \n",
    "\n",
    "    random_tweet_= get_random_tweet(n, n_gram_models_test)\n",
    "    print(\"test: \", random_tweet_)\n",
    "    classifyresult = classify_fn(n,random_tweet_ , n_gram_models1, n_gram_models2)\n",
    "    print(\"classify result \", classifyresult)\n",
    "    ### END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "4f5f88a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start  ['RealDonaldTrump']\n",
      "test:  RealDonaldTrump in the race and shut down the opposition Im ready for America to be the next POTUS # Trump Thanks for the spirit </s>\n",
      "Token: RealDonaldTrump, Context: [], Ratio: 0.0\n",
      "Token: in, Context: ['RealDonaldTrump'], Ratio: 0.0\n",
      "Token: the, Context: ['RealDonaldTrump', 'in'], Ratio: 0.0\n",
      "Token: race, Context: ['RealDonaldTrump', 'in', 'the'], Ratio: 0.0\n",
      "Token: and, Context: ['in', 'the', 'race'], Ratio: -20.125428842099883\n",
      "Token: shut, Context: ['the', 'race', 'and'], Ratio: -1.098612282001443\n",
      "Token: down, Context: ['race', 'and', 'shut'], Ratio: 0.0\n",
      "Token: the, Context: ['and', 'shut', 'down'], Ratio: 18.420680743952367\n",
      "Token: opposition, Context: ['shut', 'down', 'the'], Ratio: -0.18232155646062123\n",
      "Token: Im, Context: ['down', 'the', 'opposition'], Ratio: 0.0\n",
      "Token: ready, Context: ['the', 'opposition', 'Im'], Ratio: 0.0\n",
      "Token: for, Context: ['opposition', 'Im', 'ready'], Ratio: 0.0\n",
      "Token: America, Context: ['Im', 'ready', 'for'], Ratio: -0.6931471855599453\n",
      "Token: to, Context: ['ready', 'for', 'America'], Ratio: 0.0\n",
      "Token: be, Context: ['for', 'America', 'to'], Ratio: 16.811242849518266\n",
      "Token: the, Context: ['America', 'to', 'be'], Ratio: -1.6094379044341005\n",
      "Token: next, Context: ['to', 'be', 'the'], Ratio: 0.022989514642510894\n",
      "Token: POTUS, Context: ['be', 'the', 'next'], Ratio: 15.81799107313761\n",
      "Token: #, Context: ['the', 'next', 'POTUS'], Ratio: -1.098612292001443\n",
      "Token: Trump, Context: ['next', 'POTUS', '#'], Ratio: 0.0\n",
      "Token: Thanks, Context: ['POTUS', '#', 'Trump'], Ratio: -1.098612292001443\n",
      "Token: for, Context: ['#', 'Trump', 'Thanks'], Ratio: -2.9444389796927566\n",
      "Token: the, Context: ['Trump', 'Thanks', 'for'], Ratio: -9.999999878202987e-09\n",
      "Token: spirit, Context: ['Thanks', 'for', 'the'], Ratio: -3.401197371995489\n",
      "Token: </s>, Context: ['for', 'the', 'spirit'], Ratio: -9.999999878202987e-09\n",
      "classify result  First with 18.82109545500363\n",
      "start  ['they']\n",
      "test:  they also need to turn a desire for change into impactapply to be an @ OFA fellow today </s>\n",
      "Token: they, Context: [], Ratio: 0.0\n",
      "Token: also, Context: ['they'], Ratio: 0.0\n",
      "Token: need, Context: ['they', 'also'], Ratio: 0.0\n",
      "Token: to, Context: ['they', 'also', 'need'], Ratio: 0.0\n",
      "Token: turn, Context: ['also', 'need', 'to'], Ratio: 0.0\n",
      "Token: a, Context: ['need', 'to', 'turn'], Ratio: 0.0\n",
      "Token: desire, Context: ['to', 'turn', 'a'], Ratio: 0.0\n",
      "Token: for, Context: ['turn', 'a', 'desire'], Ratio: 0.0\n",
      "Token: change, Context: ['a', 'desire', 'for'], Ratio: 0.0\n",
      "Token: into, Context: ['desire', 'for', 'change'], Ratio: 0.0\n",
      "Token: impactapply, Context: ['for', 'change', 'into'], Ratio: 0.0\n",
      "Token: to, Context: ['change', 'into', 'impactapply'], Ratio: 0.0\n",
      "Token: be, Context: ['into', 'impactapply', 'to'], Ratio: 0.0\n",
      "Token: an, Context: ['impactapply', 'to', 'be'], Ratio: 0.0\n",
      "Token: @, Context: ['to', 'be', 'an'], Ratio: 20.53089394604138\n",
      "Token: OFA, Context: ['be', 'an', '@'], Ratio: 18.420680743952367\n",
      "Token: fellow, Context: ['an', '@', 'OFA'], Ratio: 16.54887857128154\n",
      "Token: today, Context: ['@', 'OFA', 'fellow'], Ratio: 17.72753356839242\n",
      "Token: </s>, Context: ['OFA', 'fellow', 'today'], Ratio: -9.999999878202987e-09\n",
      "classify result  First with 73.2279868196677\n"
     ]
    }
   ],
   "source": [
    "context_length = 3\n",
    "validate(context_length, trump_tweets_train, biden_tweets_train, classify_fn=classify)\n",
    "validate(context_length, obama_tweets_train, biden_tweets_train, classify_fn=classify)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298e1fb7-c67b-4e44-87c7-488e704c5ac1",
   "metadata": {},
   "source": [
    "### Compute Perplexities\n",
    "\n",
    "4.1 Compute (and plot) the perplexities for each of the test tweets and models. Is picking the Model with minimum perplexity a better classifier than in 3.1?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "9bfe2154-d816-442e-8de8-3b836ab0ffe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def classify_with_perplexity(n, tokens, n_gram_models1, n_gram_models2):\n",
    "    \"\"\"\n",
    "    Checks which of the two given datasets is more likely for the given Tweet\n",
    "    using perplexity. Lower perplexity indicates a better model likelihood.\n",
    "    \n",
    "    Returns:\n",
    "      A string: \"First with perplexity ...\" if the first model is more likely,\n",
    "      otherwise \"Second with perplexity ...\".\n",
    "    \"\"\"\n",
    "    model1 = n_gram_models1[n]\n",
    "    model2 = n_gram_models2[n]\n",
    "    smoothing = 1e-8\n",
    "    log_sum1 = 0.0\n",
    "    log_sum2 = 0.0\n",
    "    count_tokens = 0\n",
    "\n",
    "    # Use a sliding context of up to (n-1) tokens.\n",
    "    for i in range(len(tokens)):\n",
    "        start = max(0, i - (n - 1))\n",
    "        context = tuple(tokens[start:i])\n",
    "        token = tokens[i]\n",
    "\n",
    "        # Probability for Model 1\n",
    "        if context in model1:\n",
    "            occ1 = model1[context]\n",
    "            count1 = occ1.count(token)\n",
    "            total1 = len(occ1)\n",
    "        else:\n",
    "            count1, total1 = 0, 0\n",
    "        prob1 = (count1 + smoothing) / (total1 + smoothing) if total1 > 0 else smoothing\n",
    "\n",
    "        # Probability for Model 2\n",
    "        if context in model2:\n",
    "            occ2 = model2[context]\n",
    "            count2 = occ2.count(token)\n",
    "            total2 = len(occ2)\n",
    "        else:\n",
    "            count2, total2 = 0, 0\n",
    "        prob2 = (count2 + smoothing) / (total2 + smoothing) if total2 > 0 else smoothing\n",
    "\n",
    "        log_sum1 += math.log(prob1)\n",
    "        log_sum2 += math.log(prob2)\n",
    "        count_tokens += 1\n",
    "\n",
    "    avg_log1 = log_sum1 / count_tokens\n",
    "    avg_log2 = log_sum2 / count_tokens\n",
    "    perplexity1 = math.exp(-avg_log1)\n",
    "    perplexity2 = math.exp(-avg_log2)\n",
    "\n",
    "    print (f\"Perplexity Model 1: {perplexity1:.4f}, Perplexity Model 2: {perplexity2:.4f}\")\n",
    "    if perplexity1 < perplexity2:\n",
    "        return f\"First with perplexity {perplexity1:.4f}\"\n",
    "    else:\n",
    "        return f\"Second with perplexity {perplexity2:.4f}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3be177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start  ['agillogly']\n",
      "test:  agillogly @ realDonaldTrump DT do you think kids are overmedicated and over diagnosed in the USA Yes </s>\n",
      "Perplexity Model 1: 100000000.0000, Perplexity Model 2: 100000000.0000\n",
      "classify result  Second with perplexity 100000000.0000\n",
      "start  ['<s>']\n"
     ]
    }
   ],
   "source": [
    "context_length = 3\n",
    "validate(context_length, trump_tweets_train, biden_tweets_train, classify_fn=classify_with_perplexity)\n",
    "validate(context_length, obama_tweets_train, biden_tweets_train, classify_fn=classify_with_perplexity)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
